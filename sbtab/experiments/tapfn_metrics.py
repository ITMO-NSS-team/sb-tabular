
from __future__ import annotations

import argparse
import json
import pickle
from dataclasses import asdict
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
from scipy.stats import wasserstein_distance
from sklearn.metrics import r2_score
from sklearn.model_selection import KFold

from sbtab.data.schema import TabularSchema
from sbtab.transforms.pipeline import TransformPipeline
from sbtab.baselines.tabpfn.model import TabPFGenGenerative, TabPFGenConfig


# ----------------------------
# Dataset -> target column map
# ----------------------------

TARGET_COL_BY_DATASET: Dict[str, str] = {
    "covertype": "Horizontal_Distance_To_Hydrology",
    "online_shoppers": "ProductRelated",
    "bank_marketing": "pdays",
    "bank_loan": "Income",
    "diabetes": "target",
    "california_housing": "MedHouseVal",
    "online_news_popularity": " shares",
    "king_county_housing": "price",
    "german_credit":'duration',
}


# ----------------------------
# Utility regressor (R2)
# ----------------------------

def make_regressor(random_state: int):
    """
    Prefer CatBoost if available; fallback to sklearn otherwise.
    """
    try:
        from catboost import CatBoostRegressor  # type: ignore
        return CatBoostRegressor(
            depth=8,
            learning_rate=0.1,
            iterations=500,
            loss_function="RMSE",
            random_seed=random_state,
            verbose=False,
        )
    except Exception:
        from sklearn.ensemble import HistGradientBoostingRegressor
        return HistGradientBoostingRegressor(
            random_state=random_state,
            max_depth=8,
            learning_rate=0.1,
            max_iter=500,
        )


# ----------------------------
# Metrics
# ----------------------------

def avg_wd(real: pd.DataFrame, synth: pd.DataFrame, cols: List[str]) -> float:
    return float(np.mean([wasserstein_distance(real[c].to_numpy(), synth[c].to_numpy()) for c in cols]))


def avg_kl_hist(
    real: pd.DataFrame,
    synth: pd.DataFrame,
    cols: List[str],
    n_bins: int = 50,
    eps: float = 1e-12,
) -> float:
    """
    Histogram-based marginal KL divergence: KL(p_real || p_synth) averaged over columns.
    Uses shared bins per feature from combined min/max.
    """
    kls: List[float] = []
    for c in cols:
        r = real[c].to_numpy()
        s = synth[c].to_numpy()

        lo = float(np.min([np.min(r), np.min(s)]))
        hi = float(np.max([np.max(r), np.max(s)]))
        if not np.isfinite(lo) or not np.isfinite(hi) or lo == hi:
            kls.append(0.0)
            continue

        bins = np.linspace(lo, hi, n_bins + 1)

        pr, _ = np.histogram(r, bins=bins, density=False)
        ps, _ = np.histogram(s, bins=bins, density=False)

        pr = pr.astype(np.float64) + eps
        ps = ps.astype(np.float64) + eps
        pr /= pr.sum()
        ps /= ps.sum()

        kls.append(float(np.sum(pr * (np.log(pr) - np.log(ps)))))

    return float(np.mean(kls))


def corr_frobenius(real: pd.DataFrame, synth: pd.DataFrame, cols: List[str]) -> float:
    rc = real[cols].corr().to_numpy()
    sc = synth[cols].corr().to_numpy()
    return float(np.linalg.norm(rc - sc, ord="fro"))


def utility_delta_r2_percent(
    train_real: pd.DataFrame,
    test_real: pd.DataFrame,
    train_synth: pd.DataFrame,
    feature_cols: List[str],
    target_col: str,
    seed: int,
) -> Tuple[float, float, float]:
    """
    Utility metric: delta% between R2 scores on real test.

    R2_real  : model trained on real-train, evaluated on real-test
    R2_synth : model trained on synth-train, evaluated on real-test

    delta% = (R2_synth - R2_real) / (abs(R2_real) + 1e-12) * 100
    """
    Xtr = train_real[feature_cols].to_numpy()
    ytr = train_real[target_col].to_numpy()
    Xte = test_real[feature_cols].to_numpy()
    yte = test_real[target_col].to_numpy()

    reg_real = make_regressor(seed)
    reg_real.fit(Xtr, ytr)
    r2_real = float(r2_score(yte, reg_real.predict(Xte)))

    Xs = train_synth[feature_cols].to_numpy()
    ys = train_synth[target_col].to_numpy()
    reg_syn = make_regressor(seed + 1)
    reg_syn.fit(Xs, ys)
    r2_syn = float(r2_score(yte, reg_syn.predict(Xte)))

    delta = (r2_syn - r2_real) / (abs(r2_real) + 1e-12) * 100.0
    return float(delta), float(r2_real), float(r2_syn)


def resolve_target_col(ds_name: str, df: pd.DataFrame, strict: bool) -> str:
    if ds_name in TARGET_COL_BY_DATASET:
        t = TARGET_COL_BY_DATASET[ds_name]
        if t not in df.columns:
            raise ValueError(
                f"Mapped target '{t}' not found in dataset '{ds_name}'. Columns: {df.columns.tolist()}"
            )
        return t

    if strict:
        raise KeyError(
            f"No target mapping for dataset '{ds_name}'. "
            f"Add it to TARGET_COL_BY_DATASET or disable --strict-targets."
        )

    # fallback
    t = df.columns[-1]
    print(f"[WARN] No target mapping for '{ds_name}'. Falling back to last column: '{t}'")
    return t


# ----------------------------
# Main
# ----------------------------

def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--pickle", type=str, default="C:/Users/Anaxagor/Documents/projects/sb-tabular/sbtab/data/datasets/datasets_continuous_only.pkl")
    ap.add_argument("--outdir", type=str, default="tabpfgen_kfold_eval")

    ap.add_argument("--datasets", type=str, default=",".join(TARGET_COL_BY_DATASET.keys()))
    ap.add_argument("--n-splits", type=int, default=5)
    ap.add_argument("--shuffle", action="store_true", default=True)
    ap.add_argument("--seed", type=int, default=42)

    # TabPFGen params
    ap.add_argument("--device", type=str, default="cuda", choices=["auto", "cpu", "cuda"])
    ap.add_argument("--n-sgld-steps", type=int, default=1000)
    ap.add_argument("--sgld-step-size", type=float, default=0.01)
    ap.add_argument("--sgld-noise-scale", type=float, default=0.01)
    ap.add_argument("--task", type=str, default="auto", choices=["auto", "regression", "classification"])
    ap.add_argument("--balance-classes", action="store_true")
    ap.add_argument("--use-quantiles", action="store_true")

    ap.add_argument("--n-bins-kl", type=int, default=20)
    ap.add_argument("--strict-targets", action="store_true",
                    help="Fail if dataset has no target mapping in TARGET_COL_BY_DATASET")

    args = ap.parse_args()

    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    with open(args.pickle, "rb") as f:
        my_data: Dict[str, pd.DataFrame] = pickle.load(f)

    if args.datasets.strip().upper() == "ALL":
        ds_list = list(my_data.keys())
    else:
        ds_list = [d.strip() for d in args.datasets.split(",") if d.strip()]
        missing = [d for d in ds_list if d not in my_data]
        if missing:
            raise KeyError(f"Missing dataset keys in pickle: {missing}")

    global_rows = []

    for ds_name in ds_list:
        print("\n" + "=" * 100)
        print(f"DATASET: {ds_name}")
        print("=" * 100)

        df = my_data[ds_name].copy()
        cols = list(df.columns)

        # Ensure numeric (continuous-only expected)
        for c in cols:
            df[c] = pd.to_numeric(df[c], errors="coerce")

        if len(cols) < 2:
            print(f"[SKIP] Dataset '{ds_name}' has <2 columns.")
            continue

        target_col = resolve_target_col(ds_name, df, strict=args.strict_targets)
        feature_cols = [c for c in cols if c != target_col]
        if len(feature_cols) < 1:
            print(f"[SKIP] Dataset '{ds_name}' has no features after selecting target '{target_col}'.")
            continue

        # KFold
        kf = KFold(n_splits=args.n_splits, shuffle=args.shuffle, random_state=args.seed)
        idx = np.arange(len(df))

        fold_rows = []

        for fold_id, (train_idx, test_idx) in enumerate(kf.split(idx)):
            print(f"\n--- Fold {fold_id+1}/{args.n_splits} ---")

            df_train_raw = df.iloc[train_idx].copy()
            df_test_raw = df.iloc[test_idx].copy()

            # Preprocess per fold: fit on train only, apply to train+test
            schema = TabularSchema(feature_cols=cols)
            pipe = TransformPipeline.default_continuous_dropna()
            pipe.fit(df_train_raw, schema)

            train_scaled = pipe.transform(df_train_raw)
            if train_scaled.shape[0] > 8000:
                train_scaled = train_scaled.sample(5000)
                train_scaled.reset_index(inplace=True)
            test_scaled = pipe.transform(df_test_raw)
            if test_scaled.shape[0] > 8000:
                test_scaled = test_scaled.sample(5000)
                test_scaled.reset_index(inplace=True)
            

            # Fit TabPFGen on train_scaled (supervised: needs target)
            gen_cfg = TabPFGenConfig(
                target_col=target_col,
                task=args.task,  # auto/regression/classification
                n_sgld_steps=int(args.n_sgld_steps),
                sgld_step_size=float(args.sgld_step_size),
                sgld_noise_scale=float(args.sgld_noise_scale),
                device=args.device,
                balance_classes=bool(args.balance_classes),
                use_quantiles=bool(args.use_quantiles),
                seed=args.seed,
            )
            gen = TabPFGenGenerative(gen_cfg).fit(train_scaled)

            # Sample n = test size
            synth_scaled = gen.sample(n=len(test_scaled), seed=args.seed + 1000 + fold_id)
            if not isinstance(synth_scaled, pd.DataFrame):
                synth_scaled = pd.DataFrame(synth_scaled, columns=cols)

            # Metrics on PREPROCESSED test fold
            m_kl = avg_kl_hist(test_scaled, synth_scaled, cols=cols, n_bins=args.n_bins_kl)
            m_wd = avg_wd(test_scaled, synth_scaled, cols=cols)
            m_corr = corr_frobenius(test_scaled, synth_scaled, cols=cols)

            # Utility on PREPROCESSED data
            util_delta, r2_real, r2_syn = utility_delta_r2_percent(
                train_real=train_scaled,
                test_real=test_scaled,
                train_synth=synth_scaled,
                feature_cols=feature_cols,
                target_col=target_col,
                seed=args.seed + fold_id,
            )

            fold_rows.append(
                {
                    "dataset": ds_name,
                    "fold": fold_id,
                    "n_train": len(train_scaled),
                    "n_test": len(test_scaled),
                    "avg_kl": float(m_kl),
                    "avg_wd": float(m_wd),
                    "corr_frob": float(m_corr),
                    "delta_r2_percent": float(util_delta),
                    "r2_real": float(r2_real),
                    "r2_synth": float(r2_syn),
                }
            )

            print(f"avg_KL={m_kl:.6f}  avg_WD={m_wd:.6f}  corr_F={m_corr:.6f}  deltaR2%={util_delta:.3f}")

        # Save per-dataset fold metrics
        fold_df = pd.DataFrame(fold_rows)
        fold_csv = outdir / f"{ds_name}_fold_metrics.csv"
        fold_df.to_csv(fold_csv, index=False)

        # Summary
        def mean_std(s: pd.Series) -> Tuple[float, float]:
            return float(s.mean()), float(s.std(ddof=0))

        summary = {
            "dataset": ds_name,
            "target_col": target_col,
            "n_splits": int(args.n_splits),
            "shuffle": bool(args.shuffle),
            "seed": int(args.seed),
            "tabpfgen_config": asdict(gen_cfg),
            "metrics_mean": {},
            "metrics_std": {},
        }

        for key in ["avg_kl", "avg_wd", "corr_frob", "delta_r2_percent", "r2_real", "r2_synth"]:
            mu, sd = mean_std(fold_df[key])
            summary["metrics_mean"][key] = mu
            summary["metrics_std"][key] = sd

        summary_json = outdir / f"{ds_name}_kfold_summary.json"
        summary_json.write_text(json.dumps(summary, indent=2), encoding="utf-8")

        global_rows.append(
            {
                "dataset": ds_name,
                "target_col": target_col,
                "avg_kl_mean": summary["metrics_mean"]["avg_kl"],
                "avg_kl_std": summary["metrics_std"]["avg_kl"],
                "avg_wd_mean": summary["metrics_mean"]["avg_wd"],
                "avg_wd_std": summary["metrics_std"]["avg_wd"],
                "corr_frob_mean": summary["metrics_mean"]["corr_frob"],
                "corr_frob_std": summary["metrics_std"]["corr_frob"],
                "delta_r2_percent_mean": summary["metrics_mean"]["delta_r2_percent"],
                "delta_r2_percent_std": summary["metrics_std"]["delta_r2_percent"],
                "r2_real_mean": summary["metrics_mean"]["r2_real"],
                "r2_synth_mean": summary["metrics_mean"]["r2_synth"],
                "fold_csv": str(fold_csv),
                "summary_json": str(summary_json),
            }
        )

        print(f"\nSaved fold metrics:   {fold_csv}")
        print(f"Saved dataset summary:{summary_json}")

    # Global summary CSV
    if global_rows:
        global_df = pd.DataFrame(global_rows).sort_values("avg_wd_mean", ascending=True)
        global_csv = outdir / "kfold_summary_all_datasets.csv"
        global_df.to_csv(global_csv, index=False)
        print("\n" + "=" * 100)
        print("DONE. Global summary saved:")
        print(global_csv)
        print("=" * 100)
    else:
        print("\nNo datasets evaluated (global_rows empty).")


if __name__ == "__main__":
    main()
